{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f374328e",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/skypilot-org/skypilot/master/docs/source/images/skypilot-wide-light-1k.png\" width=500>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e849f4e",
   "metadata": {},
   "source": [
    "# Running batch inference using LLMs ðŸ¤–ï¸\n",
    "\n",
    "SkyPilot has made running batch inference using LLMs super easy. In this tutorial, we will run offline batch inference on a TODO(skycamp) model on heterogeneous GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843c58dd",
   "metadata": {},
   "source": [
    "# Learning outcomes ðŸŽ¯\n",
    "\n",
    "After completing this notebook, you will be able to:\n",
    "\n",
    "1. List the GPUs and Accelerators supported by SkyPilot. \n",
    "2. Specify different resource types (NVIDIA and AMD GPUs) for your LLM batch inference.\n",
    "3. Submit a task to run batch inference on any cloud.\n",
    "4. Use SkyPilot managed jobs to save up to 3x of your cloud costs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b3aac4",
   "metadata": {},
   "source": [
    "# Specifying resource requirements of tasks\n",
    "\n",
    "Special resource requirements are specified through the `resources` field in the SkyPilot task YAML. For example, to request 1 L4 GPU for your task, simply add it to the YAML like so:\n",
    "\n",
    "```yaml\n",
    "resources:\n",
    "  accelerators: L4:1\n",
    "\n",
    "setup: ....\n",
    "\n",
    "run: .....\n",
    "```\n",
    "\n",
    "> **ðŸ’¡ Hint -** In addition to `accelerators`, you can specify many more requirements, such as `disk_size`, a specific `infra`, `instance_type` and more! You can find more details in the [YAML configuration docs](https://skypilot.readthedocs.io/en/latest/reference/yaml-spec.html).\n",
    "\n",
    "SkyPilot even supports specifying multiple GPUs of different types, across different platforms! For example, to request 1 L4 or 1 MI300, you can do the following to seamlessly switch between them:\n",
    "\n",
    "```yaml\n",
    "resources:\n",
    "  any_of:\n",
    "    - cloud: kubernetes\n",
    "      image_id: docker:rocm/vllm:latest\n",
    "      accelerators: MI300:1\n",
    "    - image_id: docker:vllm/vllm-openai:latest\n",
    "      accelerators: L4:1\n",
    "```\n",
    "\n",
    "> **ðŸ’¡ Hint -** Notice that different accelerators might need different runtime images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b100bc08",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">[DIY]</span> ðŸ’» Launch your LLM batch inference task!\n",
    "\n",
    "In this tutorial, we will run batch inference on `Qwen/Qwen3-4B-Instruct-2507` model. The model will be run on either 1 L4 GPU from Nvidia or 1 MI300 GPU from AMD. Depending on the availability of the resources, SkyPilot will automatically choose the best option for you.\n",
    "\n",
    "**Open a terminal and use `sky launch` to create a GPU cluster:**\n",
    "\n",
    "-------------------------\n",
    "```console\n",
    "sky launch -c llm-$SKYPILOT_USER_ID batch_inference.yaml\n",
    "```\n",
    "-------------------------\n",
    "\n",
    "This will take about 2 minutes.\n",
    "\n",
    "### Expected output\n",
    "\n",
    "SkyPilot will automatically failover through all locations in Kubernetes and other clouds to find available resources, and you will see output like:\n",
    "\n",
    "-------------------------\n",
    "```console\n",
    "root@c4784b202994:/skycamp-tutorial/02_batch_inference# sky launch -c llm-$SKYPILOT_USER_ID batch_inference.yaml\n",
    "YAML to run: batch_inference.yaml\n",
    "Considered resources (1 node):\n",
    "-----------------------------------------------------------------------------------------------------------\n",
    " INFRA                                    INSTANCE         vCPUs   Mem(GB)   GPUS      COST ($)   CHOSEN   \n",
    "-----------------------------------------------------------------------------------------------------------\n",
    " Kubernetes (amd-devclou...8s-skycamp1)   -                16      64        MI300:1   0.00          âœ”     \n",
    " Kubernetes (amd-devclou...8s-skycamp2)   -                16      64        MI300:1   0.00                \n",
    " Kubernetes (amd-devclou...8s-skycamp3)   -                16      64        MI300:1   0.00                \n",
    " Kubernetes (amd-devclou...8s-skycamp4)   -                16      64        MI300:1   0.00                \n",
    " GCP (us-east4-a)                         g2-standard-16   16      64        L4:1      1.14                \n",
    " AWS (us-east-1)                          g6.4xlarge       16      64        L4:1      1.32                \n",
    "-----------------------------------------------------------------------------------------------------------\n",
    "Launching a new cluster 'llm-user_2'. Proceed? [Y/n]: \n",
    "âš™ï¸Ž Launching on Kubernetes.\n",
    "â””â”€â”€ Pod is up.\n",
    "âœ“ Cluster launched: llm-user_2.  View logs: sky logs --provision llm-user_2\n",
    "âš™ï¸Ž Syncing files.\n",
    "Multiple resources are specified for the task, using: Kubernetes(cpus=16, mem=64+, {'MI300': 1}, image_id=docker:rocm/vllm:latest)\n",
    "âš™ï¸Ž Job submitted, ID: 1\n",
    "â”œâ”€â”€ Waiting for task resources on 1 node.\n",
    "â””â”€â”€ Job started. Streaming logs... (Ctrl-C to exit log streaming; job will not be killed)\n",
    "(task, pid=3791) INFO 11-20 04:50:24 [__init__.py:225] Automatically detected platform rocm.\n",
    "(task, pid=3791) INFO 11-20 04:50:27 [run_batch.py:30] vLLM batch processing API version 0.11.1rc2.dev141+g38f225c2a.rocm700\n",
    "...\n",
    "(task, pid=3791) INFO 11-20 04:50:27 [run_batch.py:44] Prometheus metrics disabled\n",
    "(task, pid=3791) INFO 11-20 04:50:34 [model.py:658] Resolved architecture: Qwen3ForCausalLM\n",
    "(task, pid=3791) INFO 11-20 04:50:34 [model.py:1745] Using max model len 4096\n",
    "(task, pid=3791) INFO 11-20 04:50:35 [scheduler.py:225] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
    "(task, pid=3791) INFO 11-20 04:50:35 [vllm.py:375] Cudagraph is disabled under eager mode\n",
    "(task, pid=3791) INFO 11-20 04:50:37 [__init__.py:225] Automatically detected platform rocm.\n",
    "(task, pid=3791) (EngineCore_DP0 pid=9398) INFO 11-20 04:50:40 [core.py:730] Waiting for init message from front-end.\n",
    "...\n",
    "(task, pid=3791) [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
    "(task, pid=3791) [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
    "(task, pid=3791) [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
    "(task, pid=3791) [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
    "(task, pid=3791) [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
    "(task, pid=3791) [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
    "(task, pid=3791) (EngineCore_DP0 pid=9398) INFO 11-20 04:50:42 [parallel_state.py:1325] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
    "(task, pid=3791) (EngineCore_DP0 pid=9398) INFO 11-20 04:50:42 [gpu_model_runner.py:2843] Starting to load model Qwen/Qwen3-4B-Instruct-2507...\n",
    "(task, pid=3791) (EngineCore_DP0 pid=9398) INFO 11-20 04:50:43 [rocm.py:298] Using Rocm Attention backend on V1 engine.\n",
    "(task, pid=3791) (EngineCore_DP0 pid=9398) WARNING 11-20 04:50:43 [compilation.py:874] Op 'quant_fp8' not present in model, enabling with '+quant_fp8' has no effect\n",
    "(task, pid=3791) (EngineCore_DP0 pid=9398) INFO 11-20 04:50:43 [weight_utils.py:419] Using model weights format ['*.safetensors']\n",
    "(task, pid=3791) (EngineCore_DP0 pid=9398) INFO 11-20 04:50:48 [weight_utils.py:440] Time spent downloading weights for Qwen/Qwen3-4B-Instruct-2507: 5.128488 seconds\n",
    "Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n",
    "Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:01<00:02,  1.45s/it]\n",
    "Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:01<00:00,  1.46it/s]\n",
    "Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:03<00:00,  1.03s/it]\n",
    "Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:03<00:00,  1.01s/it]\n",
    "(task, pid=3791) (EngineCore_DP0 pid=9398) \n",
    "(task, pid=3791) (EngineCore_DP0 pid=9398) INFO 11-20 04:50:51 [default_loader.py:314] Loading weights took 3.14 seconds\n",
    "(task, pid=3791) (EngineCore_DP0 pid=9398) INFO 11-20 04:50:51 [gpu_model_runner.py:2904] Model loading took 7.6719 GiB and 8.817031 seconds\n",
    "(task, pid=3791) (EngineCore_DP0 pid=9398) [aiter] type hints mismatch, override to --> rmsnorm2d_fwd(input: torch.Tensor, weight: torch.Tensor, epsilon: float, use_model_sensitive_rmsnorm: int = 0) -> torch.Tensor\n",
    "(task, pid=3791) (EngineCore_DP0 pid=9398) [2025-11-20 04:50:52] WARNING core.py:622: type hints mismatch, override to --> rmsnorm2d_fwd(input: torch.Tensor, weight: torch.Tensor, epsilon: float, use_model_sensitive_rmsnorm: int = 0) -> torch.Tensor\n",
    "(task, pid=3791) (EngineCore_DP0 pid=9398) INFO 11-20 04:51:06 [gpu_worker.py:315] Available KV cache memory: 163.45 GiB\n",
    "(task, pid=3791) (EngineCore_DP0 pid=9398) INFO 11-20 04:51:06 [kv_cache_utils.py:1199] GPU KV cache size: 1,190,224 tokens\n",
    "(task, pid=3791) (EngineCore_DP0 pid=9398) INFO 11-20 04:51:06 [kv_cache_utils.py:1204] Maximum concurrency for 4,096 tokens per request: 290.58x\n",
    "(task, pid=3791) (EngineCore_DP0 pid=9398) INFO 11-20 04:51:06 [core.py:243] init engine (profile, create kv cache, warmup model) took 14.75 seconds\n",
    "(task, pid=3791) (EngineCore_DP0 pid=9398) INFO 11-20 04:51:07 [vllm.py:375] Cudagraph is disabled under eager mode\n",
    "(task, pid=3791) (EngineCore_DP0 pid=9398) INFO 11-20 04:51:07 [gc_utils.py:40] GC Debug Config. enabled:False,top_objects:-1\n",
    "(task, pid=3791) INFO 11-20 04:51:07 [loggers.py:191] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 74389\n",
    "(task, pid=3791) INFO 11-20 04:51:07 [run_batch.py:366] Supported tasks: ['generate']\n",
    "(task, pid=3791) WARNING 11-20 04:51:07 [model.py:1602] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.\n",
    "(task, pid=3791) INFO 11-20 04:51:07 [serving_chat.py:130] Using default chat sampling params from model: {'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}\n",
    "(task, pid=3791) INFO 11-20 04:51:07 [run_batch.py:419] Reading batch from https://raw.githubusercontent.com/cblmemo/skycamp25-tutorial/refs/heads/main/02_batch_inference/batch_input.jsonl...\n",
    "Running batch:   0% Completed | 0/6 [00:00<?, ?req/s]\n",
    "(task, pid=3791) INFO 11-20 04:51:08 [chat_utils.py:546] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n",
    "Running batch: 100% Completed | 6/6 [00:04<00:00,  1.38req/s]\n",
    "(task, pid=3791) \n",
    "(task, pid=3791) INFO 11-20 04:51:11 [run_batch.py:272] Writing outputs to local file results.jsonl\n",
    "(task, pid=3791) [rank0]:[W1120 04:51:11.741131699 ProcessGroupNCCL.cpp:1522] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
    "...\n",
    "âœ“ Job finished (status: SUCCEEDED).\n",
    "\n",
    "ðŸ“‹ Useful Commands\n",
    "Job ID: 1\n",
    "â”œâ”€â”€ To cancel the job:          sky cancel llm-user_2 1\n",
    "â”œâ”€â”€ To stream job logs:         sky logs llm-user_2 1\n",
    "â””â”€â”€ To view job queue:          sky queue llm-user_2\n",
    "Cluster name: llm-user_2\n",
    "â”œâ”€â”€ To log into the head VM:    ssh llm-user_2\n",
    "â”œâ”€â”€ To submit a job:            sky exec llm-user_2 yaml_file\n",
    "â”œâ”€â”€ To stop the cluster:        sky stop llm-user_2\n",
    "â””â”€â”€ To teardown the cluster:    sky down llm-user_2\n",
    "root@c4784b202994:/skycamp-tutorial/02_batch_inference#\n",
    "```\n",
    "-------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2bd451",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">[DIY]</span> ðŸ’» Terminate your batch inference cluster!\n",
    "Don't forget to terminate it! Use `sky down` to terminate a cluster.\n",
    "\n",
    "**Run `sky down` to terminate the cluster**\n",
    "\n",
    "-------------------------\n",
    "```console\n",
    "sky down llm-$SKYPILOT_USER_ID\n",
    "```\n",
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8bf61e",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">[DIY]</span> ðŸ’» Save the cost by 3x with managed spot job!\n",
    "\n",
    "To use managed spot to use your model with a 3x cost reduction, simply switch the job launch command to `sky jobs launch --use-spot`:\n",
    "\n",
    "-------------------------\n",
    "```console\n",
    "sky jobs launch --use-spot batch_inference.yaml -n batch-inference\n",
    "```\n",
    "-------------------------\n",
    "\n",
    "SkyPilot will automatically recover the job whenever preemption happens. Since our task is periodically checkpointed to the cloud bucket, the recovery will only experience limited progress loss.\n",
    "\n",
    "### Expected output\n",
    "\n",
    "You will see a similar output as before, but with a 3x cost reduction!\n",
    "\n",
    "```console\n",
    "root@c4784b202994:/skycamp-tutorial/02_batch_inference# sky jobs launch --use-spot batch_inference.yaml -n batch-inference\n",
    "YAML to run: batch_inference.yaml\n",
    "Managed job 'batch-inference' will be launched on (estimated):\n",
    "No resource satisfying Kubernetes([Spot], cpus=16, mem=64+, {'MI300': 1}, image_id=docker:rocm/vllm:latest) on Kubernetes.\n",
    "- Try specifying a different CPU count, or add \"+\" to the end of the CPU count to allow for larger instances.\n",
    "- Try specifying a different memory size, or add \"+\" to the end of the memory size to allow for larger instances.\n",
    "Kubernetes: The following features are not supported by Kubernetes:\n",
    "        Feature        Reason                                           \n",
    "        spot_instance  Spot instances are not supported in Kubernetes.  \n",
    "Considered resources (1 node):\n",
    "-----------------------------------------------------------------------------------------------\n",
    " INFRA                     INSTANCE               vCPUs   Mem(GB)   GPUS   COST ($)   CHOSEN   \n",
    "-----------------------------------------------------------------------------------------------\n",
    " AWS (ap-northeast-2d)     g6.4xlarge[Spot]       16      64        L4:1   0.25          âœ”     \n",
    " GCP (asia-northeast3-a)   g2-standard-16[Spot]   16      64        L4:1   0.30                \n",
    "-----------------------------------------------------------------------------------------------\n",
    "ðŸ” Multiple AWS instances satisfy L4:1. The cheapest [spot](gpus=L4:1, g6.4xlarge, ...) is considered among: g6.4xlarge, gr6.4xlarge.\n",
    "To list more details, run: sky show-gpus L4\n",
    "\n",
    "Launching a managed job 'batch-inference'. Proceed? [Y/n]: \n",
    "Launching managed job 'batch-inference'...\n",
    "â”œâ”€â”€ Waiting for task resources on 1 node.\n",
    "â””â”€â”€ Job started. Streaming logs... (Ctrl-C to exit log streaming; job will not be killed)\n",
    "...\n",
    "âœ“ Job finished (status: SUCCEEDED).\n",
    "âœ“ Managed job finished: 1 (status: SUCCEEDED).\n",
    "\n",
    "root@c4784b202994:/skycamp-tutorial/02_batch_inference# \n",
    "```\n",
    "\n",
    "> **ðŸ’¡ Hint** - For detailed information on how to develop, train and serve LLMs, please checkout the [examples](https://github.com/skypilot-org/skypilot/tree/master/llm) in SkyPilot repository."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9688d199",
   "metadata": {},
   "source": [
    "#### ðŸŽ‰ Congratulations! You have learnt how to run LLMs with SkyPilot!\n",
    "\n",
    "Feel free to explore more use cases in our [repository](https://github.com/skypilot-org/skypilot), [blog](https://blog.skypilot.co/), and [documentation](https://skypilot.readthedocs.io/en/latest/). \n",
    "\n",
    "Weâ€™d love to hear from you â€” join our community on Slack: [slack.skypilot.co](slack.skypilot.co).\n",
    "\n",
    "#### Quick Survey for Today's Event\n",
    "\n",
    "Weâ€™d love your feedback! Please take a moment to fill out our [quick survey](https://tinyurl.com/skypilot-survey-skycamp25) ðŸ™Œ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
